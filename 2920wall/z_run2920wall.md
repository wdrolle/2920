# How It Works

## Server Side
- **Model Loading & Listening**: The FastAPI server loads the model and listens for POST requests at the "/generate/" endpoint.
- **Request Handling**: Upon receiving a request, it employs the model to **generate text** based on the provided context.

## Run the Server

To start the FastAPI server, run the following command in your terminal:

```bash
uvicorn server:app --reload
```

## Client Side
- **Sending Requests**: The client script sends a POST request to the FastAPI server, providing the context for text generation.
- **Receiving Responses**: It then receives the generated text in response.

## Advantages of Using FastAPI
- **Asynchronous Support**: FastAPI's support for asynchronous request handling makes it highly efficient for IO-bound operations.
- **Swagger Documentation**: The framework automatically generates interactive API documentation using Swagger UI.
- **Data Validation**: Utilizes Pydantic for rigorous data validation, ensuring the integrity of data received at endpoints.

## Notes for Deployment
- **Network Accessibility**: Verify that both the server and client scripts are accessible within your network, especially if they are running on different machines.
- **Production Deployment**: For a more robust deployment, consider using servers like Hypercorn or Gunicorn with Uvicorn workers. Additionally, prioritize securing your API to safeguard against potential vulnerabilities.

## How the Refactored Script Works

- **Send Request Function**: `send_request_to_llm` sends a POST request to the FastAPI server with the given context and handles the server's response.

- **Generate from String**: `generate_text_from_string` takes a string and sends it to the LLM for processing.

- **Generate from File**: `generate_text_from_file` reads the content of the specified file and sends it to the LLM.

- **User Choices**: The `main` function provides users with options to either input text directly or specify a file path for text generation.

- **Error Handling**: The script includes basic error handling for file reading.

### Usage

1. Run the script, and it will prompt you to choose between asking a question or processing a file.
2. Depending on your choice, either input your text or specify the file path.
3. The script then displays the result obtained from the LLM.

This setup should provide the flexibility you need to interact with your LLM either by inputting text directly or by processing the contents of a file.

## Explanation

### `generate_assessment` Function
This function now expects the LLM to provide both an assessment and a rationale for the risk level. The rationale is expected to be part of the text generated by the LLM. We assume the LLM's output contains the assessment followed by a newline and then the rationale.

### `assess_risk_with_llm` Function
This function has been modified to parse the assessment text into two parts: the actual risk level assessment and the rationale. It uses a dictionary `risk_descriptions` to map the risk score to its corresponding description. If the LLM fails to provide a clear rationale, a default message is added.

### Risk Descriptions
These are mapped according to the score, and the corresponding description is added to the `Risk` field in the output data.

### Error Handling
If the LLM does not identify a risk level or fails to provide a rationale, default messages are included to indicate these cases.

With these modifications, your output data will now include detailed information about the risk level, the score, the rationale behind the score, and other related details, providing a comprehensive view of each assessment made by the LLM.

---

## Range of Temperature: 0.0 to 1.0+

Commonly used values are in the range from 0.7 to 1.0, but it can go above 1.0 for more randomness.

### Lowest Setting (Temperature close to 0.0)
At a very low temperature (approaching 0.0), the model becomes deterministic and repetitive. It tends to choose the most likely next word with very high probability. This results in text that is highly predictable and lacks creativity or variation.

### Middle Setting (Temperature around 0.7 to 1.0)
At a moderate temperature, the model strikes a balance between randomness and likelihood. It will choose words that are likely but also introduces variability and creativity in the generated text. This is often the preferred range for many applications as it provides coherent yet diverse outputs.

### Highest Setting (Temperature above 1.0)
As the temperature increases above 1.0, the model starts favoring less likely words, introducing a higher degree of randomness and unpredictability in the text generation. This can lead to more novel and creative outputs but can also reduce coherence and relevance to the input prompt.

---

## Range of top_p: 0 to 1

### Lowest Setting (top_p close to 0)
The model becomes highly deterministic, choosing the most likely next word almost every time. The generated text tends to be very predictable and might lack diversity. This setting is seldom used because it can make the model's output repetitive and less interesting.

### Middle Setting (top_p around 0.5)
The model balances between randomness and likelihood, focusing on the top 50% probable words. This setting often results in coherent and somewhat predictable text while still allowing for some variation and creativity. It's a common setting for many applications where a balance is needed between random creativity and logical coherence.

### Highest Setting (top_p close to 1)
The model considers a wider range of words, including less probable ones, increasing randomness in the text generation. The output can be more diverse and creative but might sometimes include less relevant or coherent parts. This setting is useful when you want to generate text with high variability and novelty.
